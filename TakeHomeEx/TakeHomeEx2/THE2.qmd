---
title: "Take-Home Exercise 1"
author:
  - name: Matthew Ho
    url: https://www.linkedin.com/in/matthewhoyiwen/
date: 02-11-2024
description: |
  Take-Home Exercise 2
categories:
  - Take-Home Exercise
format:
  html:
    toc: true
execute: 
  eval: true
  echo: true
  warning: false
---

## 1.0 Overview

## 2.0 Wrangling

Loading the required packages

1.  **sf** Needed to handle spatial data through the new simple features standard

2.  **tmap** Create thematic maps, particularly chloropleth maps in our case

3.  **tidyverse** For easy data manipulation and some visualisation

4.  **ggplot2** A step above the usual visualisations, like histograms

5.  **smoothr** I use it to remove hole in geometry

6.  **lubridate** Makes handling dates easy, great for dealing with epidemiology weeks

7.  **sfdep** Spatial dependence with spatial features, the highlight of this take home exercise. The spacetime object is particularly useful

```{r}
pacman::p_load(sf, tmap, tidyverse, ggplot2, smoothr, lubridate, sfdep)
```

Loading the TAINAN_VILLAGE dataset

```{r}
twv <- st_read(dsn = "data/geospatial", 
                 layer = "TAINAN_VILLAGE")
```

Loading the Dengue Daily apspatial dataset

```{r}
dengued <- read_csv("data/aspatial/Dengue_Daily.csv")
```

Unfortunately, I cannot read and understand Chinese characters. Leaving Chinese characters and words in my data would make work very difficult and error prone, so my first priority is to extract the right columns and translate them.

I extract the needed rows and rename them.

```{r}
dengued <- dengued[, c(1, 10, 11)]
names(dengued)
```

```{r}
names(dengued) <- c("Onset", "X", "Y")
names(dengued)
```

```{r}
head(dengued)
```

Before extracting those columns, I also wanted to see the types of additional information useful to our analysis. Some notable considerations are as follows.

-   Serotype

    -   Arguably the most interesting, unfortunately, there were too many missing values

-   Gender

-   Age group

-   Number of cases

I had some issues loading coordinates in this dataset, but setting this solved the issue for me.

```{r}
options(digits = 12)
```

```{r}
dengued[, c(2, 3)] <- lapply(dengued[, c(2, 3)], as.numeric)
head(dengued)
```

As expected, there are NA values in the dataset. I remove them here and check that they are gone.

```{r}
sum(apply(dengued, 1, function(x) any(is.na(x))))
```

```{r}
dengued <- na.omit(dengued)
```

```{r}
sum(apply(dengued, 1, function(x) any(is.na(x))))
```

I check the CRS for the TAINAN VILLAGE dataset so I know which CRS to use when converting the Dengue dataset's latlongs.

```{r}
st_crs(twv)
```

It's 3824, so I convert the dengue dataset's latlongs to match.

```{r}
dengued_sf <- st_as_sf(dengued, coords = c("X", "Y"),
                      crs = 3824)
st_crs(dengued_sf)
```

Looks good to proceed. I now narrow down the study area to the given counties.

```{r}
twvsz <- twv[twv$TOWNID %in% c("D01", "D02", "D04", "D06", "D07", "D08", "D32", "D39"), ] %>% 
  subset(select = -NOTE)
```

Visualising the study area

```{r}
plot(twvsz)
```

I want to ensure that all of the points from the dengue dataset fall within a polygon and not within a small gap. I use st_union to check for any holes.

```{r}
u_twvsz <- st_union(twvsz)
plot(u_twvsz)
```

Unfortunately, there were quite a few small slithers and holes. However, they seemed quite small.

```{r}
unh_twvsz <- fill_holes(u_twvsz, units::set_units(1, "km^2"))
diff_twvsz <- st_difference(unh_twvsz, u_twvsz)
plot(diff_twvsz)
```

To ensure that we were not losing any data or jeopardising the accuracy of our analysis by expanding polygons to fill the holes, I wanted to check how many points fell within the holes.

Fortunately, we did not have any, meaning I did not have to modify the geometry.

```{r}
hole_victims <- st_intersection(dengued_sf, diff_twvsz)
head(hole_victims)
```

I restrict the data range to epidemiology weeks 31-50 2023.

Epi weeks 31-50 2023: 30-07-23 to 16-12-23

```{r}
dengued_sf_epiweeks <- dengued_sf %>% filter(Onset >= as.Date("2023-07-30") & Onset <= as.Date("2023-12-16"))
```

I now use st_intersection to restrict the points to those that fall within our study area. To reduce processing time, I did this after narrowing down our date range.

```{r}
dengue_sf <- st_intersection(dengued_sf_epiweeks, u_twvsz)
```

I then add the epi_week column according to the onset date.

```{r}
dengue_sf$epi_week <- epiweek(dengue_sf$Onset)
```

Even though the processing was quite performant, I save the preprocessed data into a .rds file.

```{r}
write_rds(dengue_sf, "data/rds/dengue_sf.rds")
```

From the below plot, we can see that most cases are in the centre, around the urban centres. The northwest region appears to be mostly agricultural areas, where people are less likely to live.

```{r}
tm_shape(u_twvsz) + 
  tm_polygons() +
tm_shape(dengue_sf) +
  tm_dots(col = "red")
```

Checking there is no duplicate geometry

```{r}
vil_dupes <- any(duplicated(twvsz$VILLCODE))
vil_dupes
```

I found a problem here.

There was a village in the northwest that did not have any dengue cases. If I were to naively combine the tainan village and dengue datasets and remove NA values, I would lose a polygon, leading to inaccurate results later on. I had to remain aware that this problematic polygon exists until I aggregate the number of dengue case, after which it could simply be 0.

Here, I use st_join to associate each dengue case with a village.

```{r}
dengue_vils_sf <- st_join(twvsz, dengue_sf)
```

Instead of simply omitting NA values, I remove only the rows without a village code, keeping the polygon with an NA value.

Below is the problematic row, which I want to keep

```{r}
dengue_vils_sf[!rownames(dengue_vils_sf) %in% rownames(na.omit(dengue_vils_sf)), ]
```

```{r}
dengue_vils_sf <- dengue_vils_sf[!is.na(dengue_vils_sf$VILLCODE), ]
```

The below code create the aggregated data by counting the rows without NA values. For the dataframe grouped by epiweeks, I insert a 0 value to preserve the geometry, which will not interfere with further analysis.

I create 2 dataframes, one grouped by village code and the other grouped by both village code and epiweek (total cases for each zone).

```{r}
dengue_vils_gb_vc <- dengue_vils_sf %>%
  group_by(VILLCODE, VILLENG) %>%
  summarise(count = sum(!is.na(epi_week)))

dengue_vils_gb_vc
```

```{r}
dengue_vils_gb_vc_epi <- dengue_vils_sf %>%
  group_by(VILLCODE, epi_week) %>%
  summarise(count = sum(!is.na(epi_week)))

dengue_vils_gb_vc_epi$epi_week <- ifelse(is.na(dengue_vils_gb_vc_epi$epi_week), 31, dengue_vils_gb_vc_epi$epi_week)

dengue_vils_gb_vc_epi
```

Here I can see that my approach worked correctly. I have preserved all polygons.

```{r}
plot(dengue_vils_gb_vc_epi)
```

Now to create the spacetime cube, I create a 0 value for each combination that does not exist, expanding on the previous NA value handling.

```{r}
template <- expand.grid(VILLCODE = unique(dengue_vils_gb_vc_epi$VILLCODE),
                        epi_week = unique(dengue_vils_gb_vc_epi$epi_week))

merged_df <- merge(template, dengue_vils_gb_vc_epi, by = c("VILLCODE", "epi_week"), all.x = TRUE)

merged_df$count[is.na(merged_df$count)] <- 0

merged_df <- select(merged_df, -geometry)

merged_df <- st_as_sf(distinct(merge(merged_df, dengue_vils_gb_vc_epi[, c("VILLCODE", "geometry")], 
                  by = "VILLCODE", suffixes = c("", ".y"), all.x = TRUE)))
```

I now check that the dimensions match up. It's not exactly cube, but a block. It matches up, and I save the processed spacetime cube.

```{r}
print(nrow(merged_df))
print(length(unique(merged_df$VILLCODE)))
print(length(unique(merged_df$epi_week)))
```

```{r}
spt <- as_spacetime(merged_df, "VILLCODE", "epi_week")
is_spacetime_cube(spt)
```

```{r}
write_rds(spt, "data/rds/spt.rds")
```

If data is full of landmines, the NA value remaining partially throughout the process was a dirty needle. It won't hurt too much immediately, but it might mess me up in the long run.

## 3.0 EDA

```{r}
set.seed(42)
```

I wanted to see how the number of new cases changed every week, taking inspiration from

https://jenpoer-is415-gaa-exercises.netlify.app/take-home-exercises/exe-02/the2#exploratory-data-analysis-eda-with-choropleth-maps

From the facets map, we can observe that cases originated from the central to southeast region before spreading and remaining the in central region. Infections then dissipated in the central regions, while some new cases kept emerging in the less central regions.

```{r}
tm_shape(dengue_vils_gb_vc_epi) +
  tm_polygons(col='white') +
tm_shape(dengue_vils_gb_vc_epi) +
  tm_polygons("count",
          palette = "Blues",
          style="quantile") +
  tm_facets(by="epi_week", free.coords = FALSE)
```

Overall, We can see that the majority of cases occurred in areas around the central and eastern regions. The central regions are likely lighter because they are smaller. Likewise, the larger central and eastern regions are darker than expected.

```{r}
tm_shape(dengue_vils_gb_vc) +
  tm_polygons("count",
          palette = "Blues",
          style="quantile")
```

```{r}
tm_shape(dengue_vils_gb_vc) +
  tm_polygons("count",
          palette = "Blues",
          style="equal")
```

## 4.0 Global Measures of Spatial Autocorrelation

In this section, I use Moran's I to ascertain the presence of systemic spatial variations of dengue cases. In other words, how the number of dengue cases in each village varies according to its surround villages.

### 4.1 Calculating Neighbours and Weights

```{r}
wm_q.nb <- st_contiguity(dengue_vils_gb_vc$geometry)
wm_q.wt <- st_weights(wm_q.nb, style = "W")
wm_q.count <- dengue_vils_gb_vc$count
```

### 4.2 Global Moran's I Test

```{r}
global_moran_test(wm_q.count,
           wm_q.nb,
           wm_q.wt,
           zero.policy = TRUE,
           na.action=na.omit)
```

From this test, the positive moran's I statistic suggests that there is clustering, or a degree of spatial autocorrelation. This is natural, since infections, including those transmitted through vectors like mosquitoes, spread to those within their surrounding. This transmission however, will be limited by the availability of vectors in addition to humans.

We can also see that the P-value is very small. From a frequentist approach, we can see that this is unlikely to have occured by chance.

To strengthen our findings, we run a monte-carlo simulation and observe the results.

```{r}
moran_mc_res = global_moran_perm(wm_q.count,
                wm_q.nb,
                wm_q.wt,
                nsim=999, 
                zero.policy = TRUE, 
                na.action=na.omit)
moran_mc_res
```

From the above results, after 1000 simulations, our observed result outranks all of the random simulations. We can see the summary statistics below. As expected, the mean, min and max are close to 0, as would be the case for random distributions.

```{r}
summary(moran_mc_res$res[1:999])
```

```{r}
var(moran_mc_res$res[1:999])
```

To visualise the monte-carlo simulation results, we plot a histogram. Our observed result was 0.468, which falls well outside the results generated from our simulation. As such, we can deem the results to be unlikely to be due to chance and that there is a significant degree of spatial autocorrelation in the number of dengue cases per village.

```{r}
ggplot() + 
  aes(moran_mc_res$res[1:999]) + 
  geom_histogram(colour="black", fill="pink") + 
  labs(title = "Histogram of Simulated Moran's I For Tainan City Dengue Cases",
       x = "Simulated Moran's I",
       y = "Occurences") +
  theme_minimal()
```

```{r}
# MI_corr <- sp.correlogram(dv_total_wm_q, 
#                           dengue_vils_gb_vc$count, 
#                           order=8, 
#                           method="I", 
#                           style="W")
# plot(MI_corr)
# print(MI_corr)
```

## 5.0 Local Measures of Spatial Autocorrelation

Local Indicators of Spatial Association, or LISA, let us evaluate clusters between regions. In simpler terms, it is a statistical method, where higher values denote that the region is more heavily influenced by its surroundings. 

It is also important to consider that not every local moran's I statistic will be statistically significant. 

Calculating local moran's I statistics and append the results to the original datafram as new columns.

```{r}
local_mi <- local_moran(wm_q.count, wm_q.nb, wm_q.wt)
dengue_vils_gb_vc <- cbind(dengue_vils_gb_vc, local_mi)
```

From class examples, I found it very intuitive to plot a p-value chloropleth map next to the local Moran's I map, since it requires me to match complex shapes mentally from one map to the other. 

To reduce the mental load, I have combined these maps after categorising the p-values.

```{r}
p_val_2_cats <- function(x) {
  if (x < 0.005) {
    return("p < 0.005")
  } else if (x < 0.01) {
    return("p < 0.01")
  } else if (x < 0.05) {
    return("p < 0.05")
  } else {
    return("p > 0.05")
  }
}

dengue_vils_gb_vc <- dengue_vils_gb_vc %>%
  mutate(p_values = sapply(p_ii, p_val_2_cats))
```

From this map, we can observe statisticaly significant spatial autocorrelation in some central regions and the north-western region. In the case of the north-western regions, the significant local Moran's I statistics tell us that for these areas with fewer infections, they were likely influenced by their neighbours' lack of infections.

This kind of analysis can also show certain villages that are "outliers" in the sense that their number of infections does not align with their neighbours'. These outliers are the statistically significant red regions. 

These outliers suggest that there may be a reason why Dengue is not transmitted between two neighbours, such as inhabitants having little reason to travel to adjacent regions, or subtle environmental barriers inhibiting the range of vectors.

TODO: Show the EDA map

```{r}
tm_shape(dengue_vils_gb_vc) +
  tm_fill(col = "ii", 
          style = "pretty",
          palette = "RdBu",
          title = "local moran statistics") +
  tm_borders(alpha = 0.5) + 
  
tm_shape(dengue_vils_gb_vc[dengue_vils_gb_vc$p_values != "p > 0.05", ]) +
  tm_symbols(col = "p_values",
             title.shape = "P Values:",
             shapes.labels = c("p < 0.05", "p < 0.01", "p < 0.005"),
             size = 0.1,
             palette=c('purple', 'hotpink', 'white'))
```

We can refine the map above to only display statistically significant results with LISA quadrants. 

```{r}
quadrant <- vector(length=nrow(dengue_vils_gb_vc))
dengue_vils_gb_vc$lag_count <- st_lag(wm_q.count, wm_q.nb, wm_q.wt)
DV <- dengue_vils_gb_vc$lag_count - mean(dengue_vils_gb_vc$lag_count)     
LM_I <- local_mi[,1]   
signif <- 0.05       
quadrant[DV <0 & LM_I>0] <- "LOW - LOW"
quadrant[DV >0 & LM_I<0] <- "LOW - HIGH"
quadrant[DV <0 & LM_I<0] <- "HIGH - LOW"  
quadrant[DV >0 & LM_I>0] <- "HIGH - HIGH"    
quadrant[local_mi[,5]>signif] <- 0

dengue_vils_gb_vc$quadrant <- quadrant
```

```{r}
lisa_sig <- dengue_vils_gb_vc  %>%
  filter(p_ii < 0.05)
tmap_mode("plot")

tm_shape(dengue_vils_gb_vc) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(lisa_sig) +
  tm_fill("quadrant") + 
  tm_borders(alpha = 0.4)
```

## 6.0 EHSA

```{r}
ggplot(spt, aes(x = epi_week, y = count)) +
  geom_histogram(binwidth = 1, fill = "skyblue", stat = "identity") +
  labs(x = "Epi Week", y = "Count") +
  ggtitle("Histogram of Epi Week")
```

```{r}
# Function to create color mapping
create_color_mapping <- function(all_breaks, all_colors, map_breaks) {
  color_mapping <- rep(NA, length(map_breaks))
  for (i in seq_along(map_breaks)) {
    match_index <- match(map_breaks[i], all_breaks)
    if (!is.na(match_index)) {
      color_mapping[i] <- all_colors[match_index]
    }
  }
  return(color_mapping)
}

# Example data
all_breaks <- c("consecutive coldspot", "consecutive hotspot", "new coldspot", "new hotspot", "no pattern detected", "intensifying coldspot", "intensifying hotspot", "oscilating coldspot", "oscilating hotspot", "persistent coldspot", "persistent hotspot", "sporadic coldspot", "sporadic hotspot"
)

all_colors <- c("#FF5733", "#66CDAA", "#BA55D3", "#20B2AA", "#DC143C", "#4169E1", "#8A2BE2", "#FFD700", "#00FF7F", "#FF4500", "#9932CC", "#32CD32", "#FF1493"
)

```

```{r}
ehsa_4_spt <- function(sptcube, week_lim, geom) {
  spt_n <- filter(spt, epi_week <= week_lim)
  
  spt_nb <- spt_n %>%
  activate("geometry") %>% 
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%  
  set_wts("wt") %>%
  set_nbs("nb")
  
  EHSA <- emerging_hotspot_analysis(
    x = spt_n, 
    .var = "count", 
    k = 1, 
    nsim = 99
  )
  
  gghist <- ggplot(data = EHSA,
       aes(x = classification)) +
  geom_bar(fill="light blue") + 
    coord_flip()
  
  twv_EHSA <- twvsz %>%
  left_join(EHSA,
            by = c("VILLCODE" = "location")) %>%
   mutate(`p_value` = replace(`p_value`, `p_value` > 0.05, NA),
          `classification` = ifelse(is.na(`p_value`), NA, `classification`))
  
  plot(gghist)
  
  color_mapping <- create_color_mapping(all_breaks, all_colors, sort(unique(twv_EHSA$classification)))
  
  tm_shape(twv_EHSA) +
    tm_fill(col = "classification", title = "Classification", palette = color_mapping) +
    tm_borders()
}
```

```{r}
ehsa_4_spt(spt, 35, twvsz)
```

```{r}
ehsa_4_spt(spt, 40, twvsz)
```

```{r}
ehsa_4_spt(spt, 45, twvsz)
```

```{r}
ehsa_4_spt(spt, 50, twvsz)
```
