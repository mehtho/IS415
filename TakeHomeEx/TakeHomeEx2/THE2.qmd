---
title: "Take-Home Exercise 1"
author:
  - name: Matthew Ho
    url: https://www.linkedin.com/in/matthewhoyiwen/
date: 02-11-2024
description: |
  Take-Home Exercise 2
categories:
  - Take-Home Exercise
format:
  html:
    toc: true
execute: 
  eval: true
  echo: true
  warning: false
---

## 1.0 Overview

### 1.1 Objectives

1.  Process the data and confine the analysis to D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan.
2.  Perform Global Spatial Autocorrelation with sfdep
3.  Perform Local Spatial Autocorrelation with sfdep
4.  Perform Emerging Hotspot Analysis with sdfep
5.  Describe the spatial patterns observed

### 1.2 Approach

1.  Besides the usual procedures like checking CRS, I will aggregate point data into counts, which will be used for Global and Spatial Autocorrelation measurements
2.  Global spatial autocorrelation should go beyond the frequentist approach
3.  Global and Spatial Autocorrelation measures can use the full range of epidemiology weeks 31-50, but EHSA should use other smaller ranges

## 2.0 Setup

### 2.1 Dependencies

Loading the required packages

1.  **sf** Needed to handle spatial data through the new simple features standard

2.  **tmap** Create thematic maps, particularly chloropleth maps in our case

3.  **tidyverse** For easy data manipulation and some visualisation

4.  **ggplot2** A step above the usual visualisations, like histograms

5.  **smoothr** I use it to remove holes in geometry

6.  **lubridate** Makes handling dates easy, great for dealing with epidemiology weeks

7.  **sfdep** Spatial dependence with spatial features, the highlight of this take home exercise. The spacetime object is particularly useful

```{r}
pacman::p_load(sf, tmap, tidyverse, ggplot2, smoothr, lubridate, sfdep)
```

### 2.2 Datasets

Loading the TAINAN_VILLAGE dataset

```{r}
twv <- st_read(dsn = "data/geospatial", 
                 layer = "TAINAN_VILLAGE")
```

Loading the Dengue Daily aspatial dataset

```{r}
dengued <- read_csv("data/aspatial/Dengue_Daily.csv")
```

#### Setting random seed

```{r}
set.seed(42)
```

## 3.0 Wrangling

### 3.1 Extracting Columns

Unfortunately, I cannot read and understand Chinese characters. Leaving Chinese characters and words in my data would make work very difficult and error prone, so my first priority is to extract the right columns and translate them.

I extract the needed rows and rename them.

```{r}
dengued <- dengued[, c(1, 10, 11)]
names(dengued)
```

```{r}
names(dengued) <- c("Onset", "X", "Y")
names(dengued)
```

```{r}
head(dengued)
```

Before extracting those columns, I also wanted to see the types of additional information useful to our analysis. Some notable considerations are as follows.

-   Serotype

    -   Arguably the most interesting, unfortunately, there were too many missing values

-   Gender

-   Age group

-   Number of cases

I had some issues loading coordinates in this dataset, but setting this solved the issue for me.

```{r}
# options(digits = 12)
```

Transforming the coordinate strings into numerical values

```{r}
dengued[, c(2, 3)] <- lapply(dengued[, c(2, 3)], as.numeric)
head(dengued)
```

### 3.2 Dealing with Missing Data

As expected, there are NA values in the dataset. I remove them here and check that they are gone.

```{r}
sum(apply(dengued, 1, function(x) any(is.na(x))))
```

```{r}
dengued <- na.omit(dengued)
```

```{r}
sum(apply(dengued, 1, function(x) any(is.na(x))))
```

### 3.3 CRS and Coordinates

I check the CRS for the TAINAN VILLAGE dataset so I know which CRS to use when converting the Dengue dataset's latlongs.

```{r}
st_crs(twv)
```

It's 3824, so I convert the dengue dataset's latlongs to match.

```{r}
dengued_sf <- st_as_sf(dengued, coords = c("X", "Y"),
                      crs = 3824)
st_crs(dengued_sf)
```

### 3.4 Study Area

Looks good to proceed. I now narrow down the study area to the given counties. I also removed the NOTE column since it did not provide much information, but kept the others. The other columns and names could help me debug or google what certain regions look like later on.

```{r}
twvsz <- twv[twv$TOWNID %in% c("D01", "D02", "D04", "D06", "D07", "D08", "D32", "D39"), ] %>% 
  subset(select = -NOTE)
```

Visualising the study area

```{r}
plot(twvsz)
```

### 3.5 Geometry Holes

I want to ensure that all of the points from the dengue dataset fall within a polygon and not within a small gap. I use st_union to check for any holes.

```{r}
u_twvsz <- st_union(twvsz)
plot(u_twvsz)
```

Unfortunately, there were quite a few small slithers and holes. However, they seemed quite small.

```{r}
unh_twvsz <- fill_holes(u_twvsz, units::set_units(1, "km^2"))
diff_twvsz <- st_difference(unh_twvsz, u_twvsz)
plot(diff_twvsz)
```

To ensure that we were not losing any data or jeopardising the accuracy of our analysis by expanding polygons to fill the holes, I wanted to check how many points fell within the holes.

Fortunately, we did not have any, meaning I did not have to modify the geometry.

```{r}
hole_victims <- st_intersection(dengued_sf, diff_twvsz)
head(hole_victims)
```

### 3.6 Epiweeks

I restrict the data range to epidemiology weeks 31-50 2023.

Epi weeks 31-50 2023: 30-07-23 to 16-12-23

```{r}
dengued_sf_epiweeks <- dengued_sf %>% filter(Onset >= as.Date("2023-07-30") & Onset <= as.Date("2023-12-16"))
```

I now use st_intersection to restrict the points to those that fall within our study area. To reduce processing time, I did this after narrowing down our date range.

```{r}
dengue_sf <- st_intersection(dengued_sf_epiweeks, u_twvsz)
```

I then add the epi_week column according to the onset date.

```{r}
dengue_sf$epi_week <- epiweek(dengue_sf$Onset)
```

From the below plot, we can see that most cases are in the centre, around the urban centres. The northwest region appears to be mostly agricultural areas, where people are less likely to live.

```{r}
tm_shape(u_twvsz) + 
  tm_polygons() +
tm_shape(dengue_sf) +
  tm_dots(col = "red")
```

#### Checking the Geometry

Checking there is no duplicate geometry

```{r}
vil_dupes <- any(duplicated(twvsz$VILLCODE))
vil_dupes
```

### 3.7 Aggregating Dengue Cases

I found a problem here.

There was a village in the northwest that did not have any dengue cases. If I were to naively combine the tainan village and dengue datasets and remove NA values, I would lose a polygon, leading to inaccurate results later on. I had to remain aware that this problematic polygon exists until I aggregate the number of dengue case, after which it could simply be 0.

Here, I use st_join to associate each dengue case with a village.

```{r}
dengue_vils_sf <- st_join(twvsz, dengue_sf)
```

Instead of simply omitting NA values, I remove only the rows without a village code, keeping the polygon with an NA value.

Below is the problematic row, which I want to keep

```{r}
dengue_vils_sf[!rownames(dengue_vils_sf) %in% rownames(na.omit(dengue_vils_sf)), ]
```

```{r}
dengue_vils_sf <- dengue_vils_sf[!is.na(dengue_vils_sf$VILLCODE), ]
```

The below code create the aggregated data by counting the rows without NA values. For the dataframe grouped by epiweeks, I insert a 0 value to preserve the geometry, which will not interfere with further analysis.

I create 2 dataframes, one grouped by village code and the other grouped by both village code and epiweek (total cases for each zone).

```{r}
dengue_vils_gb_vc <- dengue_vils_sf %>%
  group_by(VILLCODE, VILLENG) %>%
  summarise(count = sum(!is.na(epi_week)))

dengue_vils_gb_vc
```

```{r}
dengue_vils_gb_vc_epi <- dengue_vils_sf %>%
  group_by(VILLCODE, epi_week) %>%
  summarise(count = sum(!is.na(epi_week)))

dengue_vils_gb_vc_epi$epi_week <- ifelse(is.na(dengue_vils_gb_vc_epi$epi_week), 31, dengue_vils_gb_vc_epi$epi_week)

dengue_vils_gb_vc_epi
```

Here I can see that my approach worked correctly. I have preserved all polygons.

```{r}
plot(dengue_vils_gb_vc_epi)
```

### 3.8 Spacetime Cube

Now to create the spacetime cube, I create a 0 value for each combination that does not exist, expanding on the previous NA value handling.

```{r}
template <- expand.grid(VILLCODE = unique(dengue_vils_gb_vc_epi$VILLCODE),
                        epi_week = unique(dengue_vils_gb_vc_epi$epi_week))

merged_df <- merge(template, dengue_vils_gb_vc_epi, by = c("VILLCODE", "epi_week"), all.x = TRUE)

merged_df$count[is.na(merged_df$count)] <- 0

merged_df <- select(merged_df, -geometry)

merged_df <- st_as_sf(distinct(merge(merged_df, dengue_vils_gb_vc_epi[, c("VILLCODE", "geometry")], 
                  by = "VILLCODE", suffixes = c("", ".y"), all.x = TRUE)))
```

I now check that the dimensions match up. It's not exactly cube, but a block. It matches up, and I save the processed spacetime cube.

```{r}
print(nrow(merged_df))
print(length(unique(merged_df$VILLCODE)))
print(length(unique(merged_df$epi_week)))
```

```{r}
spt <- as_spacetime(merged_df, "VILLCODE", "epi_week")
is_spacetime_cube(spt)
```

```{r}
write_rds(spt, "data/rds/spt.rds")
```

If data is full of landmines, the NA value remaining partially throughout the process was a dirty needle. It won't hurt too much immediately, but it might mess me up in the long run.

## 4.0 EDA

I wanted to see how the number of new cases changed every week, taking inspiration from

https://jenpoer-is415-gaa-exercises.netlify.app/take-home-exercises/exe-02/the2#exploratory-data-analysis-eda-with-choropleth-maps

From the facets map, we can observe that cases originated from the central to southeast region before spreading and remaining the in central region. Infections then dissipated in the central regions, while some new cases kept emerging in the less central regions.

### 4.1 Cases by Epidemiology Week

```{r}
tm_shape(dengue_vils_gb_vc_epi) +
  tm_polygons(col='white') +
tm_shape(dengue_vils_gb_vc_epi) +
  tm_polygons("count",
          palette = "Blues",
          style="quantile") +
  tm_facets(by="epi_week", free.coords = FALSE)
```

### 4.2 Overall Cases

Overall, We can see that the majority of cases occurred in areas around the central and eastern regions. The central regions are likely lighter because they are smaller. Likewise, the larger central and eastern regions are darker than expected.

::: panel-tabset
## Quantile

```{r}
tm_shape(dengue_vils_gb_vc) +
  tm_polygons("count",
          palette = "Blues",
          style="quantile")
```

## Equal

```{r}
tm_shape(dengue_vils_gb_vc) +
  tm_polygons("count",
          palette = "Blues",
          style="equal")
```
:::

## 5.0 Global Measures of Spatial Autocorrelation

To assess spatial autocorrelation in our dataset, or how the presence of dengue cases in a region may form clusters.

### 5.1 Calculating Neighbours and Weights

I decided to use Queen's neighbour criteria. This should also account for polygons that only share a corner. In realistic cases, polygons that share a corner are very likely to be close enough to influence each other.

```{r}
wm_q.nb <- st_contiguity(dengue_vils_gb_vc, queen=TRUE)
wm_q.wt <- st_weights(wm_q.nb, style = "W")
wm_q.count <- dengue_vils_gb_vc$count
```

### 5.2 Global Moran's I Test

In this section, I use Moran's I to ascertain the presence of systemic spatial variations of dengue cases. In other words, how the number of dengue cases in each village varies according to its surrounding villages compared to that under spatial randomness.

```{r}
global_moran_test(wm_q.count,
           wm_q.nb,
           wm_q.wt,
           zero.policy = TRUE,
           na.action=na.omit)
```

From this test, the positive moran's I statistic suggests that there is clustering, or a degree of spatial autocorrelation. This is natural, since infections, including those transmitted through vectors like mosquitoes, spread to those within their surrounding. This transmission however, will be limited by the availability of vectors in addition to humans.

We can also see that the P-value is very small. From a frequentist approach, we can see that this is unlikely to have occured by chance.

To strengthen our findings, we run a monte-carlo simulation and observe the results.

```{r}
moran_mc_res = global_moran_perm(wm_q.count,
                wm_q.nb,
                wm_q.wt,
                nsim=999, 
                zero.policy = TRUE, 
                na.action=na.omit)
moran_mc_res
```

From the above results, after 1000 simulations, our observed result outranks all of the random simulations. We can see the summary statistics below. As expected, the mean, min and max are close to 0, as would be the case for random distributions.

```{r}
summary(moran_mc_res$res[1:999])
```

```{r}
var(moran_mc_res$res[1:999])
```

To visualise the monte-carlo simulation results, we plot a histogram. Our observed result was 0.468, which falls well outside the results generated from our simulation. As such, we can deem the results to be unlikely to be due to chance and that there is a significant degree of spatial autocorrelation in the number of dengue cases per village.

```{r}
ggplot() + 
  aes(moran_mc_res$res[1:999]) + 
  geom_histogram(colour="black", fill="pink") + 
  labs(title = "Histogram of Simulated Moran's I For Tainan City Dengue Cases",
       x = "Simulated Moran's I",
       y = "Occurences") +
  theme_minimal()
```

```{r}
# MI_corr <- sp.correlogram(dv_total_wm_q, 
#                           dengue_vils_gb_vc$count, 
#                           order=8, 
#                           method="I", 
#                           style="W")
# plot(MI_corr)
# print(MI_corr)
```

## 6.0 Local Measures of Spatial Autocorrelation

Local Indicators of Spatial Association, or LISA, let us evaluate clusters between regions. In simpler terms, it is a statistical method, where higher values denote that the region is more heavily influenced by its surroundings.

### 6.1 Local Moran's I

It is also important to consider that not every local Moran's I statistic will be statistically significant.

Calculating local Moran's I statistics and append the results to the original dataframe as new columns.

```{r}
local_mi <- local_moran(wm_q.count, wm_q.nb, wm_q.wt)
dengue_vils_gb_vc <- cbind(dengue_vils_gb_vc, local_mi)
```

From class examples, I found it very unintuitive to plot a p-value chloropleth map next to the local Moran's I map, since it requires me to match complex shapes mentally from one map to the other.

To reduce the mental load, I have combined these maps after categorising the p-values.

```{r}
p_val_2_cats <- function(x) {
  if (x < 0.005) {
    return("p < 0.005")
  } else if (x < 0.01) {
    return("p < 0.01")
  } else if (x < 0.05) {
    return("p < 0.05")
  } else {
    return("p > 0.05")
  }
}

dengue_vils_gb_vc <- dengue_vils_gb_vc %>%
  mutate(p_values = sapply(p_ii, p_val_2_cats))
```

### 6.2 Local Moran's I Plots

From this map, we can observe statistically significant spatial autocorrelation in some central regions and the north-western region. In the case of the north-western regions, the significant local Moran's I statistics tell us that for these areas with fewer infections, they were likely influenced by their neighbours' lack of infections.

This kind of analysis can also show certain villages that are "outliers" in the sense that their number of infections does not align with their neighbours'. These outliers are the statistically significant red regions.

These outliers suggest that there may be a reason why Dengue is not transmitted between two neighbours, such as inhabitants having little reason to travel to adjacent regions, or subtle environmental barriers inhibiting the range of vectors.

TODO: Show the EDA map

```{r}
tm_shape(dengue_vils_gb_vc) +
  tm_fill(col = "ii", 
          style = "pretty",
          palette = "RdBu",
          title = "local moran statistics") +
  tm_borders(alpha = 0.5) + 
  
tm_shape(dengue_vils_gb_vc[dengue_vils_gb_vc$p_values != "p > 0.05", ]) +
  tm_symbols(col = "p_values",
             title.shape = "P Values:",
             shapes.labels = c("p < 0.05", "p < 0.01", "p < 0.005"),
             size = 0.1,
             palette=c('purple', 'hotpink', 'white'))
```

### 6.3 LISA Quadrant Map

We can refine the map above to only display statistically significant results with LISA quadrants.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

quadrant <- vector(length=nrow(dengue_vils_gb_vc))
dengue_vils_gb_vc$lag_count <- st_lag(wm_q.count, wm_q.nb, wm_q.wt)
DV <- dengue_vils_gb_vc$lag_count - mean(dengue_vils_gb_vc$lag_count)     
LM_I <- local_mi[,1]   
signif <- 0.05       
quadrant[DV <0 & LM_I>0] <- "LOW - LOW"
quadrant[DV >0 & LM_I<0] <- "LOW - HIGH"
quadrant[DV <0 & LM_I<0] <- "HIGH - LOW"  
quadrant[DV >0 & LM_I>0] <- "HIGH - HIGH"    
quadrant[local_mi[,5]>signif] <- 0

dengue_vils_gb_vc$quadrant <- quadrant
```

```{r}
lisa_sig <- dengue_vils_gb_vc  %>%
  filter(p_ii < 0.05)
tmap_mode("plot")

tm_shape(dengue_vils_gb_vc) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(lisa_sig) +
  tm_fill("quadrant") + 
  tm_borders(alpha = 0.4)
```

## 7.0 Hot and Cold Spot Analysis

```{r}
hcsa <- dengue_vils_gb_vc %>% 
  cbind(local_gstar_perm(wm_q.count, wm_q.nb, wm_q.wt, nsim=99)) %>%
  mutate("p_sim" = replace(`p_sim`, `p_sim` > 0.05, NA),
         "gi_star" = ifelse(is.na(`p_sim`), NA, `gi_star`))
```

```{r}
tmap_mode("plot")
tm_shape(hcsa) +
  tm_fill("gi_star", palette="PuOr", midpoint=0, title="Gi*") + 
  tm_borders(alpha = 0.5)
```

## 7.0 Emerging Hotspot Analysis

In this section, we can analyse hot and spot spots, as well which type of hot/cold spot they are.

### 7.1 Choosing Time Horizons

However, something I noticed was that as region's hot/cold spots may change over time. This is natural since the calculations are done based on the data points provided.

For example, a region could start as a new hotspot before becoming an oscillating hotspot.

Based on the histogram below, I set points in time to conduct EHSA based on the progress of this dengue outbreak.

-   Epiweek 35: Noticeable increase in cases
-   Epiweek 39: The peak in cases
-   Epiweek 45: Midway through the decline in cases
-   Epiweek 50: When cases have dropped significantly (All of the spacetime object)

```{r}
ggplot(spt, aes(x = epi_week, y = count)) +
  geom_histogram(binwidth = 1, fill = "skyblue", stat = "identity") +
  labs(x = "Epi Week", y = "Count") +
  ggtitle("Histogram of Epi Week")
```

This is a chunk of code and function to ensure consistent coloring for each kind of hot/cold spot, making it easier to compare across maps.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

create_color_mapping <- function(all_breaks, all_colors, map_breaks) {
  color_mapping <- rep(NA, length(map_breaks))
  for (i in seq_along(map_breaks)) {
    match_index <- match(map_breaks[i], all_breaks)
    if (!is.na(match_index)) {
      color_mapping[i] <- all_colors[match_index]
    }
  }
  return(color_mapping)
}

all_breaks <- c("consecutive coldspot", "consecutive hotspot", "new coldspot", "new hotspot", "no pattern detected", "intensifying coldspot", "intensifying hotspot", "oscilating coldspot", "oscilating hotspot", "persistent coldspot", "persistent hotspot", "sporadic coldspot", "sporadic hotspot"
)

all_colors <- c("#FF5733", "#66CDAA", "#BA55D3", "#20B2AA", "#DC143C", "#4169E1", "#8A2BE2", "#FFD700", "#00FF7F", "#FF4500", "#9932CC", "#32CD32", "#FF1493"
)

```

The rule of "Do not repeat yourself" applies in R too. I have learnt from my previous takehome exercise and will use functions for repeated analysis to avoid duplicated code.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

ehsa_4_spt <- function(sptcube, week_lim, geom) {
  spt_n <- filter(spt, epi_week <= week_lim)
  
  spt_nb <- spt_n %>%
  activate("geometry") %>% 
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%  
  set_wts("wt") %>%
  set_nbs("nb")
  
  EHSA <- emerging_hotspot_analysis(
    x = spt_n, 
    .var = "count", 
    k = 1, 
    nsim = 99
  )
  
  gghist <- ggplot(data = EHSA,
       aes(x = classification)) +
  geom_bar(fill="light blue") + 
    coord_flip()
  
  twv_EHSA <- twvsz %>%
  left_join(EHSA,
            by = c("VILLCODE" = "location")) %>%
   mutate(`p_value` = replace(`p_value`, `p_value` > 0.05, NA),
          `classification` = ifelse(is.na(`p_value`), NA, `classification`))
  
  plot(gghist)
  
  color_mapping <- create_color_mapping(all_breaks, all_colors, sort(unique(twv_EHSA$classification)))
  
  tm_shape(twv_EHSA) +
    tm_fill(col = "classification", title = "Classification", palette = color_mapping) +
    tm_borders()
}
```

### 7.2 EHSA Maps

From the below visualisations, we can see that during week 40, there were some clear consecutive and new hotspots around the peripheral areas of the central area. We could also clearly see the northwestern region becoming cold spots through the outbreak.

::: panel-tabset
## EW 30-35

```{r}
ehsa_4_spt(spt, 35, twvsz)
```

## EW 30-40

```{r}
ehsa_4_spt(spt, 39, twvsz)
```

## EW 30-45

```{r}
ehsa_4_spt(spt, 45, twvsz)
```

## EW 30-50

```{r}
ehsa_4_spt(spt, 50, twvsz)
```
:::

## 8.0 Conclusions

## 9.0 Takeaways
